{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score/gpt-3.5-turbo-0125-BAML/multiple_function_score.json\n",
      "Results: multiple_function 11\n",
      "score/gpt-3.5-turbo-0125-BAML/simple_score.json\n",
      "Results: simple 21\n",
      "score/gpt-3.5-turbo-0125-BAML/parallel_function_score.json\n",
      "Results: parallel_function 19\n",
      "score/gpt-3.5-turbo-0125-BAML/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 22\n",
      "score/gpt-3.5-turbo-0125-BAML/relevance_score.json\n",
      "Results: relevance 43\n",
      "score/gpt-3.5-turbo-0125-FC/multiple_function_score.json\n",
      "Results: multiple_function 12\n",
      "score/gpt-3.5-turbo-0125-FC/simple_score.json\n",
      "Results: simple 44\n",
      "score/gpt-3.5-turbo-0125-FC/parallel_function_score.json\n",
      "Results: parallel_function 23\n",
      "score/gpt-3.5-turbo-0125-FC/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 32\n",
      "score/gpt-3.5-turbo-0125-FC/relevance_score.json\n",
      "Results: relevance 233\n",
      "No result file found for gpt-3.5-turbo-0125-FC-strict multiple_function\n",
      "No result file found for gpt-3.5-turbo-0125-FC-strict simple\n",
      "No result file found for gpt-3.5-turbo-0125-FC-strict parallel_function\n",
      "No result file found for gpt-3.5-turbo-0125-FC-strict parallel_multiple_function\n",
      "No result file found for gpt-3.5-turbo-0125-FC-strict relevance\n",
      "score/gpt-3.5-turbo-0125/multiple_function_score.json\n",
      "Results: multiple_function 51\n",
      "score/gpt-3.5-turbo-0125/simple_score.json\n",
      "Results: simple 69\n",
      "score/gpt-3.5-turbo-0125/parallel_function_score.json\n",
      "Results: parallel_function 46\n",
      "score/gpt-3.5-turbo-0125/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 64\n",
      "score/gpt-3.5-turbo-0125/relevance_score.json\n",
      "Results: relevance 86\n",
      "score/gpt-4o-2024-05-13-BAML/multiple_function_score.json\n",
      "Results: multiple_function 8\n",
      "score/gpt-4o-2024-05-13-BAML/simple_score.json\n",
      "Results: simple 19\n",
      "score/gpt-4o-2024-05-13-BAML/parallel_function_score.json\n",
      "Results: parallel_function 15\n",
      "score/gpt-4o-2024-05-13-BAML/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 21\n",
      "score/gpt-4o-2024-05-13-BAML/relevance_score.json\n",
      "Results: relevance 27\n",
      "score/gpt-4o-2024-05-13-FC/multiple_function_score.json\n",
      "Results: multiple_function 21\n",
      "score/gpt-4o-2024-05-13-FC/simple_score.json\n",
      "Results: simple 48\n",
      "score/gpt-4o-2024-05-13-FC/parallel_function_score.json\n",
      "Results: parallel_function 21\n",
      "score/gpt-4o-2024-05-13-FC/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 31\n",
      "score/gpt-4o-2024-05-13-FC/relevance_score.json\n",
      "Results: relevance 61\n",
      "score/gpt-4o-2024-05-13-FC-strict/multiple_function_score.json\n",
      "Results: multiple_function 21\n",
      "score/gpt-4o-2024-05-13-FC-strict/simple_score.json\n",
      "Results: simple 23\n",
      "score/gpt-4o-2024-05-13-FC-strict/parallel_function_score.json\n",
      "Results: parallel_function 18\n",
      "score/gpt-4o-2024-05-13-FC-strict/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 33\n",
      "score/gpt-4o-2024-05-13-FC-strict/relevance_score.json\n",
      "Results: relevance 42\n",
      "score/gpt-4o-2024-05-13/multiple_function_score.json\n",
      "Results: multiple_function 22\n",
      "score/gpt-4o-2024-05-13/simple_score.json\n",
      "Results: simple 34\n",
      "score/gpt-4o-2024-05-13/parallel_function_score.json\n",
      "Results: parallel_function 37\n",
      "score/gpt-4o-2024-05-13/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 73\n",
      "score/gpt-4o-2024-05-13/relevance_score.json\n",
      "Results: relevance 51\n",
      "score/claude-3-haiku-20240307-BAML/multiple_function_score.json\n",
      "Results: multiple_function 12\n",
      "score/claude-3-haiku-20240307-BAML/simple_score.json\n",
      "Results: simple 22\n",
      "score/claude-3-haiku-20240307-BAML/parallel_function_score.json\n",
      "Results: parallel_function 25\n",
      "score/claude-3-haiku-20240307-BAML/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 23\n",
      "score/claude-3-haiku-20240307-BAML/relevance_score.json\n",
      "Results: relevance 64\n",
      "score/claude-3-haiku-20240307-FC/multiple_function_score.json\n",
      "Results: multiple_function 24\n",
      "score/claude-3-haiku-20240307-FC/simple_score.json\n",
      "Results: simple 106\n",
      "score/claude-3-haiku-20240307-FC/parallel_function_score.json\n",
      "Results: parallel_function 199\n",
      "score/claude-3-haiku-20240307-FC/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 199\n",
      "score/claude-3-haiku-20240307-FC/relevance_score.json\n",
      "Results: relevance 185\n",
      "No result file found for claude-3-haiku-20240307-FC-strict multiple_function\n",
      "No result file found for claude-3-haiku-20240307-FC-strict simple\n",
      "No result file found for claude-3-haiku-20240307-FC-strict parallel_function\n",
      "No result file found for claude-3-haiku-20240307-FC-strict parallel_multiple_function\n",
      "No result file found for claude-3-haiku-20240307-FC-strict relevance\n",
      "score/claude-3-haiku-20240307/multiple_function_score.json\n",
      "Results: multiple_function 16\n",
      "score/claude-3-haiku-20240307/simple_score.json\n",
      "Results: simple 24\n",
      "score/claude-3-haiku-20240307/parallel_function_score.json\n",
      "Results: parallel_function 30\n",
      "score/claude-3-haiku-20240307/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 84\n",
      "score/claude-3-haiku-20240307/relevance_score.json\n",
      "Results: relevance 158\n",
      "score/gpt-4o-mini-2024-07-18-BAML/multiple_function_score.json\n",
      "Results: multiple_function 11\n",
      "score/gpt-4o-mini-2024-07-18-BAML/simple_score.json\n",
      "Results: simple 25\n",
      "score/gpt-4o-mini-2024-07-18-BAML/parallel_function_score.json\n",
      "Results: parallel_function 24\n",
      "score/gpt-4o-mini-2024-07-18-BAML/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 20\n",
      "score/gpt-4o-mini-2024-07-18-BAML/relevance_score.json\n",
      "Results: relevance 21\n",
      "score/gpt-4o-mini-2024-07-18-FC/multiple_function_score.json\n",
      "Results: multiple_function 128\n",
      "score/gpt-4o-mini-2024-07-18-FC/simple_score.json\n",
      "Results: simple 188\n",
      "score/gpt-4o-mini-2024-07-18-FC/parallel_function_score.json\n",
      "Results: parallel_function 95\n",
      "score/gpt-4o-mini-2024-07-18-FC/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 165\n",
      "score/gpt-4o-mini-2024-07-18-FC/relevance_score.json\n",
      "Results: relevance 52\n",
      "No result file found for gpt-4o-mini-2024-07-18-FC-strict multiple_function\n",
      "No result file found for gpt-4o-mini-2024-07-18-FC-strict simple\n",
      "No result file found for gpt-4o-mini-2024-07-18-FC-strict parallel_function\n",
      "No result file found for gpt-4o-mini-2024-07-18-FC-strict parallel_multiple_function\n",
      "No result file found for gpt-4o-mini-2024-07-18-FC-strict relevance\n",
      "score/gpt-4o-mini-2024-07-18/multiple_function_score.json\n",
      "Results: multiple_function 13\n",
      "score/gpt-4o-mini-2024-07-18/simple_score.json\n",
      "Results: simple 27\n",
      "score/gpt-4o-mini-2024-07-18/parallel_function_score.json\n",
      "Results: parallel_function 31\n",
      "score/gpt-4o-mini-2024-07-18/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 20\n",
      "score/gpt-4o-mini-2024-07-18/relevance_score.json\n",
      "Results: relevance 58\n",
      "score/claude-3-5-sonnet-20240620-BAML/multiple_function_score.json\n",
      "Results: multiple_function 7\n",
      "score/claude-3-5-sonnet-20240620-BAML/simple_score.json\n",
      "Results: simple 19\n",
      "score/claude-3-5-sonnet-20240620-BAML/parallel_function_score.json\n",
      "Results: parallel_function 14\n",
      "score/claude-3-5-sonnet-20240620-BAML/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 17\n",
      "score/claude-3-5-sonnet-20240620-BAML/relevance_score.json\n",
      "Results: relevance 37\n",
      "score/claude-3-5-sonnet-20240620-FC/multiple_function_score.json\n",
      "Results: multiple_function 28\n",
      "score/claude-3-5-sonnet-20240620-FC/simple_score.json\n",
      "Results: simple 66\n",
      "score/claude-3-5-sonnet-20240620-FC/parallel_function_score.json\n",
      "Results: parallel_function 86\n",
      "score/claude-3-5-sonnet-20240620-FC/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 94\n",
      "score/claude-3-5-sonnet-20240620-FC/relevance_score.json\n",
      "Results: relevance 47\n",
      "No result file found for claude-3-5-sonnet-20240620-FC-strict multiple_function\n",
      "No result file found for claude-3-5-sonnet-20240620-FC-strict simple\n",
      "No result file found for claude-3-5-sonnet-20240620-FC-strict parallel_function\n",
      "No result file found for claude-3-5-sonnet-20240620-FC-strict parallel_multiple_function\n",
      "No result file found for claude-3-5-sonnet-20240620-FC-strict relevance\n",
      "score/claude-3-5-sonnet-20240620/multiple_function_score.json\n",
      "Results: multiple_function 9\n",
      "score/claude-3-5-sonnet-20240620/simple_score.json\n",
      "Results: simple 20\n",
      "score/claude-3-5-sonnet-20240620/parallel_function_score.json\n",
      "Results: parallel_function 15\n",
      "score/claude-3-5-sonnet-20240620/parallel_multiple_function_score.json\n",
      "Results: parallel_multiple_function 16\n",
      "score/claude-3-5-sonnet-20240620/relevance_score.json\n",
      "Results: relevance 38\n",
      "No result file found for ollama-llama3.1-BAML multiple_function\n",
      "No result file found for ollama-llama3.1-BAML simple\n",
      "No result file found for ollama-llama3.1-BAML parallel_function\n",
      "No result file found for ollama-llama3.1-BAML parallel_multiple_function\n",
      "No result file found for ollama-llama3.1-BAML relevance\n",
      "No result file found for ollama-llama3.1-FC multiple_function\n",
      "No result file found for ollama-llama3.1-FC simple\n",
      "No result file found for ollama-llama3.1-FC parallel_function\n",
      "No result file found for ollama-llama3.1-FC parallel_multiple_function\n",
      "No result file found for ollama-llama3.1-FC relevance\n",
      "No result file found for ollama-llama3.1-FC-strict multiple_function\n",
      "No result file found for ollama-llama3.1-FC-strict simple\n",
      "No result file found for ollama-llama3.1-FC-strict parallel_function\n",
      "No result file found for ollama-llama3.1-FC-strict parallel_multiple_function\n",
      "No result file found for ollama-llama3.1-FC-strict relevance\n",
      "No result file found for ollama-llama3.1 multiple_function\n",
      "No result file found for ollama-llama3.1 simple\n",
      "No result file found for ollama-llama3.1 parallel_function\n",
      "No result file found for ollama-llama3.1 parallel_multiple_function\n",
      "No result file found for ollama-llama3.1 relevance\n",
      "Answer keys: {'id', 'answer', 'ground_truth'}\n",
      "Result keys: {'latency', 'id', 'input_token_count', 'output_token_count', 'prompt', 'result', 'error'}\n",
      "Score keys: {'id', 'model_result', 'decoded_result', 'possible_answer', 'valid', 'model_result_raw', 'test_category', 'error', 'error_type', 'model_name', 'model_result_decoded'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import typing\n",
    "TestType = typing.Literal['multiple_function', 'simple', 'parallel_function', 'parallel_multiple_function', 'relevance']\n",
    "\n",
    "answer_keys = set()\n",
    "result_keys = set()\n",
    "score_keys = set()\n",
    "\n",
    "def merge_results(model: str, test: TestType):\n",
    "    # results/baml-gpt-3.5-turbo-0125/gorilla_openfunctions_v1_test_simple_result.json\n",
    "    # berkeley-function-call-leaderboard/results/baml-gpt-3.5-turbo-0125/gorilla_openfunctions_v1_test_simple_result.json does\n",
    "    # berkeley-function-call-leaderboard/result/baml-gpt-3.5-turbo-0125/gorilla_openfunctions_v1_test_simple_result.json\n",
    "    if not os.path.exists(f'result/{model}/gorilla_openfunctions_v1_test_{test}_result.json'):\n",
    "        print(f'No result file found for {model} {test}')\n",
    "        return None\n",
    "\n",
    "    results = pd.read_json(f'result/{model}/gorilla_openfunctions_v1_test_{test}_result.json', lines=True)\n",
    "    \n",
    "    with open(f'score/{model}/{test}_score.json', 'r') as f:\n",
    "        content = [json.loads(line) for idx, line in enumerate(f) if idx > 0]\n",
    "        # convert to DataFrame\n",
    "        scores = pd.DataFrame(content)\n",
    "\n",
    "    if 'executable' in test or 'relevance' in test:\n",
    "        answers = [{ \"id\": f'{test}_{idx}', \"answer\": '<look-at-code>'} for idx in range(len(results))]\n",
    "    else:\n",
    "        with open(f'data/possible_answer/gorilla_openfunctions_v1_test_{test}.json', 'r') as f:\n",
    "            answers = [json.loads(line) for line in f]\n",
    "            # convert to DataFrame\n",
    "    answers = pd.DataFrame(answers)\n",
    "    answer_keys.update(answers.columns)\n",
    "    \n",
    "    # merge on results[idx], scores[id - 1] (scores is sparse)\n",
    "    # print('-----')\n",
    "    result_keys.update(results.columns)\n",
    "    # Rname result['error'] -> result['baml_error']\n",
    "    results.rename(columns={'error': 'baml_error'}, inplace=True)\n",
    "    # print('-----')\n",
    "    print(f'score/{model}/{test}_score.json')\n",
    "    print('Results:', test, len(scores))\n",
    "    if scores.empty:\n",
    "        merged = results\n",
    "        merged['_merge'] = 'left_only'\n",
    "    else:\n",
    "        # scores['id'] = scores['id'] - 1\n",
    "        # Drop prompt in scores\n",
    "        # print(scores.columns)\n",
    "        if \"relevance\" in test:\n",
    "            scores['id'] = scores['id'].apply(lambda x: f'{test}_{x - 1}')\n",
    "        else:\n",
    "            scores['id'] = scores['id'].apply(lambda x: f'{test}_{x - 1}')\n",
    "        if 'prompt' in scores.columns:\n",
    "            scores.drop(columns=['prompt'], inplace=True)\n",
    "        score_keys.update(scores.columns)\n",
    "        merged = results.merge(scores, left_on='id', right_on='id', how='outer', indicator=True)\n",
    "    merged = merged.merge(answers, left_on='id', right_on='id')\n",
    "\n",
    "\n",
    "    # print('>>>>>>>>>>>>')\n",
    "    merged['valid'] = merged['_merge'] == 'left_only'\n",
    "    # drop columns\n",
    "    merged.drop(columns=['_merge'], inplace=True)\n",
    "    merged['test_type'] = test\n",
    "    if model.endswith('-BAML'):\n",
    "        qualifier = 'baml'\n",
    "        model = model[:-5]\n",
    "    elif model.endswith('-FC'):\n",
    "        qualifier = 'FC'\n",
    "        model = model[:-3]\n",
    "    elif model.endswith('-FC-strict'):\n",
    "        qualifier = 'FC-strict'\n",
    "        model = model[:-10]\n",
    "    else:\n",
    "        qualifier = 'bfcl'\n",
    "        model = model\n",
    "    merged['qualifier'] = qualifier\n",
    "    merged['model'] = model\n",
    "    merged['test_group'] = 'exec' if 'executable' in test else 'ast'\n",
    "    \n",
    "\n",
    "    # reorder columns from current:\n",
    "    # idx\tresult\tprompt\tinput_token_count\toutput_token_count\tlatency\tvalid\terror\terror_type\tmodel_result_raw\tmodel_result_decoded\tanswer\ttest_type\n",
    "    # to:\n",
    "    # idx test_type model result answer valid error error_type model_result_raw model_result_decoded prompt input_token_count output_token_count latency\n",
    "    # merged = merged[['idx', 'test_type', 'model', 'result', 'answer', 'valid', 'error', 'raw_llm_response',  'error_type', 'prompt', 'input_token_count', 'output_token_count', 'latency']]\n",
    "\n",
    "    return merged\n",
    "\n",
    "dfs = []\n",
    "for model in ['gpt-3.5-turbo-0125', 'gpt-4o-2024-05-13', 'claude-3-haiku-20240307', 'gpt-4o-mini-2024-07-18', 'claude-3-5-sonnet-20240620', 'ollama-llama3.1']:\n",
    "    for qualifier in ['-BAML', '-FC', '-FC-strict', '']:\n",
    "        for prefix in ['']:\n",
    "            for test in ['multiple_function', 'simple', 'parallel_function', 'parallel_multiple_function', 'relevance']:\n",
    "                df = merge_results(f'{model}{qualifier}', f'{prefix}{test}')\n",
    "                if df is not None:\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Stack\n",
    "df = pd.concat(dfs)\n",
    "df.to_json('score/merged.json', orient='records', lines=True)\n",
    "\n",
    "print('Answer keys:', answer_keys)\n",
    "print('Result keys:', result_keys)\n",
    "print('Score keys:', score_keys)\n",
    "\n",
    "# copy a file to the right place\n",
    "!cp score/merged.json /Users/vbv/repos/baml-berkeley-benchmark/baml-berkeley-benchmark/public/merged.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['result', 'input_token_count', 'output_token_count', 'latency',\n",
       "       'prompt', 'model_name', 'test_category', 'valid', 'error', 'error_type',\n",
       "       'model_result_raw', 'model_result_decoded', 'possible_answer',\n",
       "       'ground_truth', 'test_type', 'qualifier', 'model', 'test_group',\n",
       "       'baml_error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_group  model               qualifier\n",
       "ast         gpt-3.5-turbo-0125  FC           0.812000\n",
       "                                baml         0.882000\n",
       "                                bfcl         0.773000\n",
       "            gpt-4o-2024-05-13   FC           0.873000\n",
       "                                baml         0.933000\n",
       "                                bfcl         0.841000\n",
       "exec        gpt-3.5-turbo-0125  FC           0.829167\n",
       "                                baml         0.895833\n",
       "                                bfcl         0.770833\n",
       "            gpt-4o-2024-05-13   FC           0.875000\n",
       "                                baml         0.900000\n",
       "                                bfcl         0.800000\n",
       "Name: valid, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mean + std of latency, group by (model, test_type, qualifier)\n",
    "\n",
    "group = df.groupby(['model', 'test_type', 'qualifier'])\n",
    "\n",
    "# Add latency = ['latency'].agg(['mean', 'std'])\n",
    "latency = group['latency'].agg(['mean', 'std'])\n",
    "latency = latency.reset_index()\n",
    "\n",
    "# Add valid_count = ['valid'].agg(['count'])\n",
    "valid_count = group['valid'].apply(lambda x: (x == True).sum() / len(x))\n",
    "valid_count = valid_count.reset_index(name='Accuracy')\n",
    "\n",
    "model_group = df.groupby(['test_group', 'model', 'qualifier'])\n",
    "\n",
    "model_valid_count = model_group['valid'].apply(lambda x: (x == True).sum() / len(x))\n",
    "# model_valid_count = model_valid_count.reset_index(name='Accuracy')\n",
    "\n",
    "\n",
    "# Function to create a radar chart\n",
    "def radar_chart(df, model, ax):\n",
    "    # Get the test types and qualifiers\n",
    "    test_types = df['test_type'].unique()\n",
    "    qualifiers = df['qualifier'].unique()\n",
    "    \n",
    "    # Number of variables\n",
    "    num_vars = len(test_types)\n",
    "\n",
    "    # Compute angle for each variable\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "\n",
    "    # Plot for each qualifier\n",
    "    for qualifier in qualifiers:\n",
    "        values = df[df['qualifier'] == qualifier]['Accuracy'].tolist()\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=qualifier)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(test_types)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(model, fontweight='bold')\n",
    "\n",
    "# Create subplots\n",
    "\n",
    "def plot(df):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot for gpt-3.5-turbo-0125\n",
    "    df_gpt35 = df[df['model'] == 'gpt-3.5-turbo-0125']\n",
    "    radar_chart(df_gpt35, 'GPT-3.5-turbo-0125', ax1)\n",
    "\n",
    "    # Plot for gpt-4o-2024-05-13\n",
    "    df_gpt4 = df[df['model'] == 'gpt-4o-2024-05-13']\n",
    "    radar_chart(df_gpt4, 'GPT-4o-2024-05-13', ax2)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "# plot(valid_count)\n",
    "\n",
    "# model_valid_count.plot(kind='bar', x='model', y='Accuracy', figsize=(10, 5))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "model_valid_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Overall Acc</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Link</th>\n",
       "      <th>Organization</th>\n",
       "      <th>License</th>\n",
       "      <th>AST Summary</th>\n",
       "      <th>Exec Summary</th>\n",
       "      <th>Simple Function AST</th>\n",
       "      <th>Python Simple Function AST</th>\n",
       "      <th>...</th>\n",
       "      <th>Python Simple Function Exec</th>\n",
       "      <th>REST Simple Function Exec</th>\n",
       "      <th>Multiple Functions Exec</th>\n",
       "      <th>Parallel Functions Exec</th>\n",
       "      <th>Parallel Multiple Exec</th>\n",
       "      <th>Relevance Detection</th>\n",
       "      <th>Cost ($ Per 1k Function Calls)</th>\n",
       "      <th>Latency Mean (s)</th>\n",
       "      <th>Latency Standard Deviation (s)</th>\n",
       "      <th>Latency 95th Percentile (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>92.66%</td>\n",
       "      <td>GPT-4o-2024-05-13 (BAML)</td>\n",
       "      <td>https://www.boundaryml.com</td>\n",
       "      <td>OpenAI + Boundary</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>92.94%</td>\n",
       "      <td>87.00%</td>\n",
       "      <td>94.75%</td>\n",
       "      <td>94.75%</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>92.00%</td>\n",
       "      <td>82.00%</td>\n",
       "      <td>75.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>88.47%</td>\n",
       "      <td>GPT-3.5-Turbo-0125 (BAML)</td>\n",
       "      <td>https://www.boundaryml.com</td>\n",
       "      <td>OpenAI + Boundary</td>\n",
       "      <td>Proprietary + Apache 2.0</td>\n",
       "      <td>86.69%</td>\n",
       "      <td>86.37%</td>\n",
       "      <td>94.25%</td>\n",
       "      <td>94.25%</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>88.00%</td>\n",
       "      <td>80.00%</td>\n",
       "      <td>77.50%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>87.34%</td>\n",
       "      <td>GPT-4o-2024-05-13 (FC)</td>\n",
       "      <td>https://openai.com/index/hello-gpt-4o/</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>87.44%</td>\n",
       "      <td>85.00%</td>\n",
       "      <td>86.75%</td>\n",
       "      <td>86.75%</td>\n",
       "      <td>...</td>\n",
       "      <td>95.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>86.00%</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>75.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>83.31%</td>\n",
       "      <td>GPT-4o-2024-05-13 (Prompt)</td>\n",
       "      <td>https://openai.com/index/hello-gpt-4o/</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>81.62%</td>\n",
       "      <td>76.50%</td>\n",
       "      <td>94.00%</td>\n",
       "      <td>94.00%</td>\n",
       "      <td>...</td>\n",
       "      <td>92.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>80.00%</td>\n",
       "      <td>64.00%</td>\n",
       "      <td>70.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81.53%</td>\n",
       "      <td>GPT-3.5-Turbo-0125 (FC)</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-...</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>82.50%</td>\n",
       "      <td>79.62%</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>...</td>\n",
       "      <td>92.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>86.00%</td>\n",
       "      <td>78.00%</td>\n",
       "      <td>62.50%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>77.26%</td>\n",
       "      <td>GPT-3.5-Turbo-0125 (Prompting)</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-...</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>75.75%</td>\n",
       "      <td>71.25%</td>\n",
       "      <td>83.50%</td>\n",
       "      <td>83.50%</td>\n",
       "      <td>...</td>\n",
       "      <td>92.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>80.00%</td>\n",
       "      <td>78.00%</td>\n",
       "      <td>35.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank Overall Acc                           Model  \\\n",
       "0     1      92.66%        GPT-4o-2024-05-13 (BAML)   \n",
       "1     2      88.47%       GPT-3.5-Turbo-0125 (BAML)   \n",
       "2     3      87.34%          GPT-4o-2024-05-13 (FC)   \n",
       "3     4      83.31%      GPT-4o-2024-05-13 (Prompt)   \n",
       "4     5      81.53%         GPT-3.5-Turbo-0125 (FC)   \n",
       "5     6      77.26%  GPT-3.5-Turbo-0125 (Prompting)   \n",
       "\n",
       "                                          Model Link       Organization  \\\n",
       "0                         https://www.boundaryml.com  OpenAI + Boundary   \n",
       "1                         https://www.boundaryml.com  OpenAI + Boundary   \n",
       "2             https://openai.com/index/hello-gpt-4o/             OpenAI   \n",
       "3             https://openai.com/index/hello-gpt-4o/             OpenAI   \n",
       "4  https://platform.openai.com/docs/models/gpt-3-...             OpenAI   \n",
       "5  https://platform.openai.com/docs/models/gpt-3-...             OpenAI   \n",
       "\n",
       "                    License AST Summary Exec Summary Simple Function AST  \\\n",
       "0               Proprietary      92.94%       87.00%              94.75%   \n",
       "1  Proprietary + Apache 2.0      86.69%       86.37%              94.25%   \n",
       "2               Proprietary      87.44%       85.00%              86.75%   \n",
       "3               Proprietary      81.62%       76.50%              94.00%   \n",
       "4               Proprietary      82.50%       79.62%              76.00%   \n",
       "5               Proprietary      75.75%       71.25%              83.50%   \n",
       "\n",
       "  Python Simple Function AST  ... Python Simple Function Exec  \\\n",
       "0                     94.75%  ...                      99.00%   \n",
       "1                     94.25%  ...                     100.00%   \n",
       "2                     86.75%  ...                      95.00%   \n",
       "3                     94.00%  ...                      92.00%   \n",
       "4                     76.00%  ...                      92.00%   \n",
       "5                     83.50%  ...                      92.00%   \n",
       "\n",
       "  REST Simple Function Exec Multiple Functions Exec Parallel Functions Exec  \\\n",
       "0                     0.00%                  92.00%                  82.00%   \n",
       "1                     0.00%                  88.00%                  80.00%   \n",
       "2                     0.00%                  86.00%                  84.00%   \n",
       "3                     0.00%                  80.00%                  64.00%   \n",
       "4                     0.00%                  86.00%                  78.00%   \n",
       "5                     0.00%                  80.00%                  78.00%   \n",
       "\n",
       "  Parallel Multiple Exec Relevance Detection Cost ($ Per 1k Function Calls)  \\\n",
       "0                 75.00%               0.00%                           2.23   \n",
       "1                 77.50%               0.00%                           0.21   \n",
       "2                 75.00%               0.00%                           1.86   \n",
       "3                 70.00%               0.00%                           2.73   \n",
       "4                 62.50%               0.00%                           0.19   \n",
       "5                 35.00%               0.00%                           0.27   \n",
       "\n",
       "  Latency Mean (s) Latency Standard Deviation (s) Latency 95th Percentile (s)  \n",
       "0             1.70                           1.29                        3.55  \n",
       "1             1.15                           0.68                        2.61  \n",
       "2             1.46                           1.01                        3.14  \n",
       "3             1.01                           0.61                        2.09  \n",
       "4             1.07                           0.52                        1.96  \n",
       "5             0.88                           1.18                        1.46  \n",
       "\n",
       "[6 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('score/data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorilla-93AzYX0D-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
